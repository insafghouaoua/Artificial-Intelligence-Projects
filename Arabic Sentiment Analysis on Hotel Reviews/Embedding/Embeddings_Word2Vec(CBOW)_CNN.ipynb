{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsabB0CSIxoP"
      },
      "source": [
        "# Étape 1: preparation des données (Tokenization + Suppression des Stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_rhoUG7Ilvz"
      },
      "source": [
        "**1. Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K8ve3EpKedS",
        "outputId": "381cb668-4a15-4e35-8e0f-356ad215da13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "# Télécharger les ressources de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtiPoG6zIrCL",
        "outputId": "42acfd0f-65d5-4767-8ecd-cb60a7f031c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   Normalized_Review  \\\n",
            "0                      ممتاز النظافة والطاقم متعاون    \n",
            "1   استثنائي سهولة إنهاء المعاملة في الاستقبال لاشيئ   \n",
            "2  استثنائي انصح بأختيار الاسويت و بالاخص غرفه رق...   \n",
            "3   استغرب تقييم الفندق كخمس نجوم لا شي يستحق  نجمه    \n",
            "4  جيد المكان جميل وهاديء كل شي جيد ونظيف بس كان ...   \n",
            "\n",
            "                                    Tokenized_Review  \n",
            "0                  [ممتاز, النظافة, والطاقم, متعاون]  \n",
            "1  [استثنائي, سهولة, إنهاء, المعاملة, في, الاستقب...  \n",
            "2  [استثنائي, انصح, بأختيار, الاسويت, و, بالاخص, ...  \n",
            "3  [استغرب, تقييم, الفندق, كخمس, نجوم, لا, شي, يس...  \n",
            "4  [جيد, المكان, جميل, وهاديء, كل, شي, جيد, ونظيف...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "file_path = '/content/hotel_reviews_preprocessed.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "data['Tokenized_Review'] = data['Normalized_Review'].apply(word_tokenize)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(data[['Normalized_Review', 'Tokenized_Review']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO96z8cOLCSo"
      },
      "source": [
        "**2. Suppression des Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW0TTFW5LKmw",
        "outputId": "68be2db4-18ff-4549-de17-8b9060039ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Télécharger Arabic stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHYvygtNK4IW",
        "outputId": "f721ede5-4eb1-48ff-d7aa-1435be53dcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Tokenized_Review  \\\n",
            "0                  [ممتاز, النظافة, والطاقم, متعاون]   \n",
            "1  [استثنائي, سهولة, إنهاء, المعاملة, في, الاستقب...   \n",
            "2  [استثنائي, انصح, بأختيار, الاسويت, و, بالاخص, ...   \n",
            "3  [استغرب, تقييم, الفندق, كخمس, نجوم, لا, شي, يس...   \n",
            "4  [جيد, المكان, جميل, وهاديء, كل, شي, جيد, ونظيف...   \n",
            "\n",
            "                                     Filtered_Review  \n",
            "0                  [ممتاز, النظافة, والطاقم, متعاون]  \n",
            "1  [استثنائي, سهولة, إنهاء, المعاملة, الاستقبال, ...  \n",
            "2  [استثنائي, انصح, بأختيار, الاسويت, بالاخص, غرف...  \n",
            "3  [استغرب, تقييم, الفندق, كخمس, نجوم, شي, يستحق,...  \n",
            "4  [جيد, المكان, جميل, وهاديء, شي, جيد, ونظيف, حو...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Importe des stopwords de NLTK\n",
        "arabic_stopwords = set(stopwords.words('arabic'))\n",
        "\n",
        "# Liste des stopwords supplémentaires\n",
        "additional_stopwords = {\n",
        "    'و', 'في', 'على', 'من', 'إلى', 'عن', 'هذا', 'هذه', 'ذلك', 'تلك',\n",
        "    'هو', 'هي', 'أنا', 'أنت', 'أنتم', 'هم', 'هن', 'كان', 'كانت', 'لك',\n",
        "    'له', 'لها', 'لنا', 'عند', 'عليه', 'عليها', 'فيها', 'بين', 'هناك',\n",
        "    'هنا', 'إذا', 'لكن', 'أو', 'بل', 'ثم', 'إن', 'أن', 'ألا', 'إلا',\n",
        "    'حتى', 'كل', 'أي', 'بعض', 'بعد', 'قبل', 'أمام', 'خلف', 'تحت', 'فوق',\n",
        "    'مع', 'لذلك', 'ذلك', 'مازال', 'قد',\n",
        "    'عندما', 'مثل', 'إليها', 'بها', 'بهم', 'منهم', 'عليهم', 'بكم', 'أيضا'\n",
        "}\n",
        "# Fusionner les listes\n",
        "arabic_stopwords.update(additional_stopwords)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in arabic_stopwords]\n",
        "\n",
        "data['Filtered_Review'] = data['Tokenized_Review'].apply(remove_stopwords)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(data[['Tokenized_Review', 'Filtered_Review']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJOQpUClJBen"
      },
      "source": [
        "# Étape  2: Génération des Embeddings avec Word2Vec (CBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6CUtLOLsdS"
      },
      "source": [
        "**1. Génération des Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Entraîner un modèle Word2Vec CBOW\n",
        "word2vec_cbow_model = Word2Vec(data['Tokenized_Review'], vector_size=100, window=5, sg=0, min_count=1)\n",
        "\n",
        "# Générer des embeddings\n",
        "def generate_word2vec_cbow_embedding(tokens):\n",
        "    embeddings = [word2vec_cbow_model.wv[word] for word in tokens if word in word2vec_cbow_model.wv]\n",
        "    return sum(embeddings) / len(embeddings) if embeddings else [0] * 100\n",
        "\n",
        "data['Embedding'] = data['Filtered_Review'].apply(generate_word2vec_cbow_embedding)\n"
      ],
      "metadata": {
        "id": "wtXAn8ahaj_o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBrDtx5wteI"
      },
      "source": [
        "**2. Enregistrement des Embeddings dans un fichier excel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "VoYIGBYkvlvD",
        "outputId": "5b4e90b3-91d5-488a-ce3e-0a615d8e4769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dd8dd734d7f9>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  output_data['Embedding'] = data['Embedding'].apply(format_embedding)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   no Hotel name  rating    user type                   room type  \\\n",
            "0   2    فندق 72       2  مسافر منفرد  غرفة ديلوكس مزدوجة أو توأم   \n",
            "1   3    فندق 72       5          زوج  غرفة ديلوكس مزدوجة أو توأم   \n",
            "2  16    فندق 72       5          زوج                           -   \n",
            "3  20    فندق 72       1          زوج          غرفة قياسية مزدوجة   \n",
            "4  23    فندق 72       4          زوج  غرفة ديلوكس مزدوجة أو توأم   \n",
            "\n",
            "            nights                                  Normalized_Review  \\\n",
            "0  أقمت ليلة واحدة                      ممتاز النظافة والطاقم متعاون    \n",
            "1  أقمت ليلة واحدة   استثنائي سهولة إنهاء المعاملة في الاستقبال لاشيئ   \n",
            "2      أقمت ليلتين  استثنائي انصح بأختيار الاسويت و بالاخص غرفه رق...   \n",
            "3  أقمت ليلة واحدة   استغرب تقييم الفندق كخمس نجوم لا شي يستحق  نجمه    \n",
            "4      أقمت ليلتين  جيد المكان جميل وهاديء كل شي جيد ونظيف بس كان ...   \n",
            "\n",
            "  Sentiment                                          Embedding  \n",
            "0  Negative  [-0.4894158542, 0.8809331656, 0.0088544376, 0....  \n",
            "1  Positive  [-0.3419909179, 0.5427557826, -0.0986717269, -...  \n",
            "2  Positive  [-0.2253079116, 0.4551826715, -0.1699838936, -...  \n",
            "3  Negative  [-0.3786251545, 0.7809402347, -0.2866847217, -...  \n",
            "4  Positive  [-0.2934971154, 0.5918660164, -0.0332046486, -...  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fc2197f3-5d43-4e3f-b9c0-02419ec8e4f6\", \"hotel_reviews_embeddings.xlsx\", 8243726)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier Excel a été sauvegardé sous le nom 'hotel_reviews_embeddings.xlsx'.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "def format_embedding(embedding):\n",
        "    return '[' + ', '.join(f'{x:.10f}' for x in embedding) + ']'\n",
        "\n",
        "# Création d'un DataFrame\n",
        "output_data = data[['no', 'Hotel name', 'rating', 'user type', 'room type', 'nights', 'Normalized_Review', 'Sentiment']]\n",
        "output_data['Embedding'] = data['Embedding'].apply(format_embedding)\n",
        "\n",
        "print(output_data.head())\n",
        "\n",
        "# Enregistrement des données dans un fichier Excel\n",
        "output_filename = '/content/hotel_reviews_embeddings.xlsx'\n",
        "output_data.to_excel(output_filename, index=False)\n",
        "\n",
        "files.download(output_filename)\n",
        "\n",
        "print(\"Le fichier Excel a été sauvegardé sous le nom 'hotel_reviews_embeddings.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9bKP9BRJI2n"
      },
      "source": [
        "# Étape 3: Entraînement d'un Classificateur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnRtAfjV4TK"
      },
      "source": [
        "**1. Entraînement du Classificateur**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKUXigs7m5-J",
        "outputId": "9b71932b-05d0-4e44-f9c6-c76a837cee08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GGMAgShL_WV",
        "outputId": "85cdcdba-4afc-4066-a1ba-e4db2ac2ae0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5821\n",
            "Epoch 2/10, Loss: 0.5204\n",
            "Epoch 3/10, Loss: 0.5222\n",
            "Epoch 4/10, Loss: 0.5069\n",
            "Epoch 5/10, Loss: 0.5104\n",
            "Epoch 6/10, Loss: 0.5041\n",
            "Epoch 7/10, Loss: 0.5019\n",
            "Epoch 8/10, Loss: 0.5008\n",
            "Epoch 9/10, Loss: 0.4963\n",
            "Epoch 10/10, Loss: 0.5002\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "file_path = '/content/hotel_reviews_embeddings.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Préparer les données\n",
        "X = data['Embedding']\n",
        "y = data['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Définir le mapping des labels\n",
        "label_mapping = {'Negative': 0, 'Positive': 1}\n",
        "y_train = [label_mapping[label] for label in y_train]\n",
        "y_test = [label_mapping[label] for label in y_test]\n",
        "\n",
        "# Dataset PyTorch\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = [eval(emb) for emb in embeddings]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return embedding, label\n",
        "\n",
        "train_dataset = SentimentDataset(X_train, y_train)\n",
        "test_dataset = SentimentDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Définir le modèle CNN\n",
        "class SentimentCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SentimentCNN, self).__init__()\n",
        "        # Reduced kernel size to 1 to match input dimension\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(1) # Reduced pooling kernel size for consistency\n",
        "        self.fc1 = nn.Linear(64, 32)  # Changed input size to 64\n",
        "        self.fc2 = nn.Linear(32, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # Permuter les dimensions pour le CNN\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "# Hyperparamètres\n",
        "input_dim = len(eval(X_train.iloc[0]))  # Dimension des embeddings\n",
        "output_dim = 2  # Binaire : Positif ou Négatif\n",
        "\n",
        "model = SentimentCNN(input_dim, output_dim)\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entraînement du modèle\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for embeddings, labels in train_loader:\n",
        "        embeddings = embeddings.unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Évaluation du model**"
      ],
      "metadata": {
        "id": "41BHxwqWcPSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL-uZfiWeWjo",
        "outputId": "d64ca73d-4ac5-4d10-e95a-844c2ce7a2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.81      0.89      0.85      1149\n",
            "    Positive       0.82      0.72      0.77       851\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.82      0.80      0.81      2000\n",
            "weighted avg       0.82      0.81      0.81      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Évaluation du modèle\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.unsqueeze(1)\n",
        "        outputs = model(embeddings)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Rapport de classification\n",
        "print(classification_report(all_labels, all_preds, target_names=['Negative',  'Positive']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Sauvegarde du Modèle**"
      ],
      "metadata": {
        "id": "4rpB8CzIgacE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_model_dir = \"/content/drive/My Drive/cnn_model\"\n",
        "os.makedirs(drive_model_dir, exist_ok=True)\n",
        "\n",
        "# Sauvegarde du modèle\n",
        "model_path = os.path.join(drive_model_dir, \"Cnn_model.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "print(f\"Le modèle CNN est sauvegardé dans : {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly8X8iRcgchg",
        "outputId": "47b032d3-2d12-468d-b26f-301f48d999ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Le modèle RNN est sauvegardé dans : /content/drive/My Drive/rnn_model/rnn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bsaM-wwJQAK"
      },
      "source": [
        "# Étape 3: Test du model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Tokenize le texte et filtre les stopwords.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in arabic_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "def generate_sentence_embedding(tokens, model):\n",
        "    \"\"\"\n",
        "    Génère un embedding pour une phrase à partir des tokens.\n",
        "    \"\"\"\n",
        "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)  # Moyenne des embeddings des mots\n",
        "    else:\n",
        "        return np.zeros(100)  # Embedding nul si aucun mot n'a un embedding\n",
        "\n",
        "def predict_sentiment(model, tokens):\n",
        "    \"\"\"\n",
        "    Prédit le sentiment à partir des embeddings générés.\n",
        "    \"\"\"\n",
        "    embedding = generate_sentence_embedding(tokens, word2vec_cbow_model)\n",
        "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Passer les embeddings dans le modèle\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(embedding_tensor)\n",
        "        prediction = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Mapping des classes\n",
        "    label_mapping = {0: \"Negative\", 1: \"Positive\"}\n",
        "    return label_mapping[prediction]\n",
        "\n",
        "# Boucle pour tester l'entrée utilisateur\n",
        "while True:\n",
        "    avis = input(\"Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : \")\n",
        "    if avis.lower() == 'quit':\n",
        "        print(\"Sortie du programme.\")\n",
        "        break\n",
        "\n",
        "    # Prétraitement du texte\n",
        "    tokens = preprocess_text(avis)\n",
        "\n",
        "    # Prédiction du sentiment\n",
        "    classe = predict_sentiment(model, tokens)\n",
        "    print(f\"L'avis est classé comme : {classe}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paytCdQcpJYz",
        "outputId": "00720833-a377-4113-e002-1e788bd8d203"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : استمتعت بكل لحظة في هذا الفندق. الإفطار كان لذيذًا والخدمات ممتازة\n",
            "L'avis est classé comme : Positive\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : الفندق قديم، يحتاج إلى تجديد\n",
            "L'avis est classé comme : Positive\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : فندق جيد\n",
            "L'avis est classé comme : Positive\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : سيء للغاية\n",
            "L'avis est classé comme : Negative\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : التكييف ضعيف للغاي\n",
            "L'avis est classé comme : Negative\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : موقع الفندق ممتاز، قريب من كل شيء\n",
            "L'avis est classé comme : Positive\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : الحمام ملوث، والخدمة سيئة\n",
            "L'avis est classé comme : Negative\n",
            "Entrez un avis en arabe pour prédire le sentiment (ou tapez 'quit' pour quitter) : quit\n",
            "Sortie du programme.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}